{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 3: Estimating a Search Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Applied Econometrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conor Bayliss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we are going to estimate the parameters of the search model for each demographic group *individually*. That is, you will *not* impose the parametrics restrictions that mapped demographics $X$ to deeper parameters using the `NamedTuples` I used last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, QuadGK, Distributions, CSV, DataFrames, DataFramesMeta, Statistics, Optim, FastGaussQuadrature, ForwardDiff, Roots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adapting Code for Automatic Differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QuadGK` doesn't play nicely with automatic differentiation since it adjusts the number of nodes adaptively. One solution is to use a fixed number of nodes and weights with `FastGaussQuadrature`. Here is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.235638422590369"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function integrandGL(f,a,b;num_nodes = 10)\n",
    "    nodes, weights = gausslegendre(num_nodes)\n",
    "    ∫f = 0.\n",
    "    for k in eachindex(nodes)\n",
    "        x = (a+b)/2 + (b-a)/2*nodes[k]\n",
    "        ∫f += weights[k]*f(x)\n",
    "    end\n",
    "    return ∫f*(b-a)/2\n",
    "end\n",
    "\n",
    "dS(x;F,β,δ) = (1-cdf(F,x)) / (1-β*(1-δ))\n",
    "res_wage(wres, b,λ,δ,β,F) = wres - b - β * λ * integrandGL(x -> dS(x;F,β,δ),wres,quantile(F,0.999))\n",
    "ForwardDiff.derivative(wres -> res_wage(wres,0.,0.5,0.03,0.99,LogNormal(0.,1.)),1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Re-writing the model solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we're going to re-write the model solution using this new integration routine. We will also use `Roots` to solve for the reservation wage in a way that will also play nicely with `ForwardDiff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4400135335598148"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_wage_solution(wres, b,λ,δ,β,F::Distribution) = wres - b - β * λ * integrandGL(x -> dS(x;F,β,δ),wres,quantile(F,0.999))\n",
    "pars = (;b=-5., λ=0.45,δ=0.03,β=0.99, F=LogNormal(1.,1.))\n",
    "\n",
    "function solve_res_wage(b,λ,δ,β,F)\n",
    "    return find_zero(wres -> res_wage_solution(wres,b,λ,δ,β,F),eltype(b)(4.))\n",
    "end\n",
    "\n",
    "solve_res_wage(0.,0.4,0.03,0.995,LogNormal())\n",
    "ForwardDiff.derivative(x -> solve_res_wage(x,0.4,0.03,0.995,LogNormal()),0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cleaning the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data cleaning is mostly the same as in **Assignment 2**. We add a function which pulls a `NamedTuple` out for a specific demographic group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(dx) = @NamedTuple{logwage::Vector{Float64}, wage_missing::BitVector, E::BitVector, tU::Vector{Float64}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "@NamedTuple{logwage::Vector{Float64}, wage_missing::BitVector, E::BitVector, tU::Vector{Float64}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = CSV.read(\"C:\\\\Users\\\\bayle\\\\Documents\\\\Github\\\\metrics\\\\hw2\\\\data\\\\cps_00019.csv\",DataFrame)\n",
    "data = @chain data begin\n",
    "    @transform :E = :EMPSTAT.<21\n",
    "    @transform @byrow :wage = begin\n",
    "        if :PAIDHOUR==0\n",
    "            return missing\n",
    "        elseif :PAIDHOUR==2\n",
    "            if :HOURWAGE<99.99 && :HOURWAGE>0\n",
    "                return :HOURWAGE\n",
    "            else\n",
    "                return missing\n",
    "            end\n",
    "        elseif :PAIDHOUR==1\n",
    "            if :EARNWEEK>0 && :UHRSWORKT<997 && :UHRSWORKT>0\n",
    "                return :EARNWEEK / :UHRSWORKT\n",
    "            else\n",
    "                return missing\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    @subset :MONTH.==1\n",
    "    @select :AGE :SEX :RACE :EDUC :wage :E :DURUNEMP\n",
    "    @transform begin\n",
    "        :bachelors = :EDUC.>=111\n",
    "        :nonwhite = :RACE.!=100 \n",
    "        :female = :SEX.==2\n",
    "        :DURUNEMP = round.(:DURUNEMP .* 12/52)\n",
    "    end\n",
    "end\n",
    "\n",
    "# the whole dataset in a named tuple\n",
    "wage_missing = ismissing.(data.wage)\n",
    "wage = coalesce.(data.wage,1.)\n",
    "N = length(data.AGE)\n",
    "X = [ones(N) data.bachelors data.female data.nonwhite]\n",
    "# create a named tuple with all variables to conveniently pass to the log-likelihood:\n",
    "d = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP, X) #<- you will need to add your demographics as well.\n",
    "\n",
    "function get_data(data,C,F,R)\n",
    "    data = @subset data :bachelors.==C :female.==F :nonwhite.==R\n",
    "    wage_missing = ismissing.(data.wage)\n",
    "    wage = coalesce.(data.wage,1.)\n",
    "    N = length(data.AGE)\n",
    "    # create a named tuple with all variables to conveniently pass to the log-likelihood:\n",
    "    return d = (;logwage = log.(wage),wage_missing,E = data.E,tU = data.DURUNEMP) \n",
    "end\n",
    "\n",
    "dx = get_data(data,1,0,0) #<- data for white men with a college degree\n",
    "@show typeof(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `dx` is our instance of the data for white men with a college degree. It is saved as a `NamedTuple`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fix $\\sigma_{\\zeta}$ (the standard deviation of measurement error in log wages) to 0.05. Following your work from last week (and recitation this week) write a function that calculates the log-likelihood of a single month of data from the CPS given $(h,\\delta,\\mu,\\sigma,w^*)$ where $w^*$ is the reservation wage and $h =$ $\\lambda$ x $(1-F_W(w^*;\\mu,\\sigma))$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define $\\phi$ and $\\Phi$, the pdf and cdf respectively of a Normal distribution with mean $\\mu$ and standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Φ (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ϕ(x,μ,σ) = pdf(Normal(μ,σ),x)\n",
    "Φ(x,μ,σ) = cdf(Normal(μ,σ),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function for the log-likelihood of observed wages. Remember that we need to integrate out measurement error. Recall that the likelihood of an observed wage $W^o$ is:\n",
    "$$\n",
    "f(W^o|E,X) = \\int_{w^*} \\frac{\\phi(\\log(w);\\mu,\\sigma)}{1-\\Phi(\\log(w^*);\\mu,\\sigma)}\\phi(\\log(W^o)-w;\\sigma_\\zeta)dw\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logwage_likelihood (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function logwage_likelihood(logwage, F::Distribution, σζ,wres)\n",
    "    f(x) = pdf(F,x) / (1-cdf(F,wres)) * ϕ(logwage,log(x),σζ) \n",
    "    ub = quantile(F,0.9999)\n",
    "    return integrandGL(f,wres,ub)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have integrated out measurement error, let us get the log-likelihhod of a single observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_likelihood (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_likelihood(d::NamedTuple, pars::NamedTuple,n)\n",
    "    (;h,σζ,wres,F,δ) = pars\n",
    "    ll = 0.\n",
    "    if d.E[n]\n",
    "        ll += log(h) - log(h+δ)\n",
    "        if !d.wage_missing[n]\n",
    "            ll += logwage_likelihood(d.logwage[n],F,σζ,wres)\n",
    "        end\n",
    "    else\n",
    "        ll += log(δ) - log(h+δ)\n",
    "        ll += log(h) + d.tU[n] * log(1-h)\n",
    "    end\n",
    "    return ll\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us write a function which maps the vector x into parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logit(x) = exp(x) / (1 + exp(x))\n",
    "logit_inv(x) = log(x/(1-x))\n",
    "function update(pars,x)\n",
    "    h = logit(x[1])\n",
    "    δ = logit(x[2])\n",
    "    μ = x[3]\n",
    "    σ = exp(x[4])\n",
    "    wres = exp(x[5])\n",
    "    F = LogNormal(μ,σ)\n",
    "    σζ = 0.05\n",
    "    β = 0.995\n",
    "    return (; pars..., h,δ,μ,σ,wres,F,σζ,β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterate over the dataset and calculate the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_likelihood_obj (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function log_likelihood_obj(d::NamedTuple, pars::NamedTuple,x)\n",
    "    pars = update(pars,x)\n",
    "    ll = 0.\n",
    "    for n in 1:length(d.logwage)\n",
    "        ll += log_likelihood(d,pars,n)\n",
    "    end\n",
    "    return ll / length(d.E)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use the log-likelihood to get maximum likelihood estimates of $(\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})$ for *white men with a college degree*. What is the advantage of estimating $h$ and $w^*$ directly instead of $\\lambda$ and $b$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass the above to `Optim`. Recall that `dx` is our instance of the data for white men with a college degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pars = (σζ = 0.05, β = 0.995, h = 0.17641764362085174, δ = 0.0038283722601471946, μ = 2.2091536420274718, σ = 1.103509994766977, wres = 21.80490807041903, F = LogNormal{Float64}(μ=2.2091536420274718, σ=1.103509994766977))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(σζ = 0.05, β = 0.995, h = 0.17641764362085174, δ = 0.0038283722601471946, μ = 2.2091536420274718, σ = 1.103509994766977, wres = 21.80490807041903, F = LogNormal{Float64}(μ=2.2091536420274718, σ=1.103509994766977))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = [logit_inv(0.5),logit_inv(0.03),2.,log(1.),log(5.)]\n",
    "pars = (;σζ=0.05,β=0.995)\n",
    "log_likelihood_obj(dx,pars,x0)\n",
    "res = optimize(x -> -log_likelihood_obj(dx,pars,x),x0,BFGS(),Optim.Options(show_trace=false))\n",
    "pars = update(pars,res.minimizer)\n",
    "@show pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, our estimated parameters are, to 5 decimal places:\n",
    "\n",
    "| Parameter | Estimate | \n",
    "| -------- | -------- | \n",
    "| $\\hat{h}$   | 0.17642    |\n",
    "| $\\hat{\\delta}$    | 0.00383    | \n",
    "| $\\hat{\\mu}$    | 2.20915    | \n",
    "| $\\hat{\\sigma}$    | 1.10351    | \n",
    "| $w^*$    | 21.80491    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Back out the implied maximum likelihood estimates of $\\hat{\\lambda}$ and $\\hat{b}$ as a function of the estimated parameters from **Part 1**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from **Part 1** that $\\lambda$ is given by:\n",
    "$$\n",
    "\\lambda = \\frac{h}{(1-F_W(w^*;\\mu,\\sigma))}\n",
    "$$\n",
    "and that we can obtain $b$ from the equation for the reservation wage:\n",
    "$$\n",
    "w^* = b + \\beta \\lambda \\int_{w^*} \\frac{1-F_W(w)}{1-\\beta(1-\\delta)}dw\n",
    "$$\n",
    "$$\n",
    "\\implies b = w^* - \\beta \\lambda \\int_{w^*} \\frac{1-F_W(w)}{1-\\beta(1-\\delta)}dw.\n",
    "$$\n",
    "The following code backs out the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "λ = 0.8226737936000635\n",
      "b = -501.51164449347914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-501.51164449347914"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "λ = pars.h / (1-cdf(pars.F,pars.wres))\n",
    "@show λ\n",
    "b = pars.wres - pars.β * λ * integrandGL(x -> dS(x;pars.F,pars.β,pars.δ),pars.wres,quantile(pars.F,0.999))\n",
    "@show b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our implicitly estimated parameters are:\n",
    "\n",
    "| Parameter | Estimate | \n",
    "| -------- | -------- | \n",
    "| $\\hat{\\lambda}$   |  0.82267    |\n",
    "| $\\hat{b}$    | -501.51164    | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Provide an estimate of the asymptotic variance of $(\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*})$ using the standard MLE formula.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the standard errors, we can take advantage of the delta method. Specifically, it tells us that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seh = 0.07870792169024328\n",
      "seδ = 0.09740231570648486\n",
      "seμ = 0.17253358400531876\n",
      "seσ = 0.042114134111141954\n",
      "sewres = 0.00843753864798779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00843753864798779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = ForwardDiff.hessian(x -> log_likelihood_obj(dx,pars,x),res.minimizer)\n",
    "N = length(dx.E)\n",
    "avar = inv(-H)\n",
    "seh = sqrt(avar[1,1] / N)\n",
    "@show seh\n",
    "seδ = sqrt.(avar[2,2] / N)\n",
    "@show seδ\n",
    "seμ = sqrt(avar[3,3] / N)\n",
    "@show seμ\n",
    "seσ = sqrt(avar[4,4] / N)\n",
    "@show seσ\n",
    "sewres = sqrt(avar[5,5] / N)\n",
    "@show sewres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can present a table of estimates and their standard errors:\n",
    "| Parameter | Estimate | Standard error |\n",
    "| -------- | -------- | -------- |\n",
    "| $\\hat{h}$    | 0.17642      | 0.07871     |\n",
    "| $\\hat{\\delta}$    | 0.00383    | 0.09740     |\n",
    "| $\\hat{\\mu}$   | 2.20915   | 0.17253    |\n",
    "| $\\hat{\\sigma}$    | 1.10351    | 0.04211 |\n",
    "| $w^*$    | 21.80491    | 0.00844 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recall that the delta method implies that if $\\hat{\\delta}$ is asymptotically normal with asymptotic variance $V$ then the vector-values function $F(\\hat{\\delta})$ is also asymptotically normal with:*\n",
    "$$\n",
    "\\sqrt{N}F((\\hat{\\theta})-F(\\theta)) \\xrightarrow{d} \\mathcal{N}(0,\\nabla_{\\theta'}FV\\nabla_{\\theta}F')\n",
    "$$\n",
    "*Use this fact to estimate the asymptotic variance of $(\\hat{h},\\hat{\\delta},\\hat{\\mu},\\hat{\\sigma},\\hat{w^*},\\hat{\\lambda},\\hat{b})$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, let us define a function `final_ests` which will take the estimated parameters and back out the other necessary parameters. We will then use it to find the Jacobian and estimate the variances using the Delta method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_ests(res.minimizer, pars) = [0.17641764362085174, 0.0038283722601471946, 2.2091536420274718, 1.103509994766977, 21.80490807041903, 0.8226737936000635, -501.51164449347914]\n",
      "se = [0.011435824872672693, 0.0003714647528974018, 0.17253358400531876, 0.04647336791260203, 0.18397975455998142, 0.14808326720908588, 25.495587911939875]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7-element Vector{Float64}:\n",
       "  0.011435824872672693\n",
       "  0.0003714647528974018\n",
       "  0.17253358400531876\n",
       "  0.04647336791260203\n",
       "  0.18397975455998142\n",
       "  0.14808326720908588\n",
       " 25.495587911939875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function final_ests(x_est, pars)\n",
    "    pars = update(pars,x_est)\n",
    "    (;h,δ,μ,σ,wres,F,β) = pars\n",
    "    λ = h / (1-cdf(F,wres))\n",
    "    b = pars.wres - β * λ * integrandGL(x -> dS(x;F,β,δ),wres,quantile(F,0.999))\n",
    "    return [h,δ,μ,σ,wres,λ,b]\n",
    "end\n",
    "@show final_ests(res.minimizer,pars)\n",
    "dF = ForwardDiff.jacobian(x -> final_ests(x,pars),res.minimizer)\n",
    "var_est = dF * avar * dF' / N\n",
    "se = sqrt.(diag(var_est))\n",
    "@show se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can now present our final results for white men with a college degree.\n",
    "\n",
    "| Parameter | Estimate | Standard error |\n",
    "| -------- | -------- | -------- |\n",
    "| $\\hat{h}$    | 0.17642      | 0.01144     |\n",
    "| $\\hat{\\delta}$    | 0.00383    | 0.00037     |\n",
    "| $\\hat{\\mu}$   | 2.20915   | 0.17253    |\n",
    "| $\\hat{\\sigma}$    | 1.10351    | 0.04647 |\n",
    "| $w^*$    | 21.80491    | 0.18398 |\n",
    "| $\\hat{\\lambda}$    | 0.82267    | 0.14808 |\n",
    "| $\\hat{b}$    | -501.51164    | 25.49559 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the inconsistencies in the standard errors between the two methods are since one uses the Hessian and the other uses the Jacobian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now report all of your estimates and standard errors for this group. Repeat this exercise for each group.*\n",
    "\n",
    "*If we thought that the parametric relationship using $\\gamma$ from **Assignment 2** described the true values of the parameters for each group, how might we use these group-specific estimates to derive estimates of each $\\gamma$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function to iterate over the relevant data subsets are obtain parameter estimates and standard errors for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.211386140082968 0.15634047422619457 … 0.16957862408630428 0.15844155895412296; 0.010965423469391606 0.012984290419368327 … 0.003016762598031792 0.004600159483590965; … ; 0.4900830220673784 0.15634047422619457 … 0.34222293919326596 0.15851295800385384; -142.31236124926681 -308.1910997484958 … -496.74313438427237 -576.7534617651261], [0.013120771099203142 0.010381427714277715 … 0.01108378446450345 0.010494743588267076; 0.0010563459343237822 0.001248278723147437 … 0.00029295321847989665 0.00044600501040462694; … ; 0.07726794508643905 0.010381427714277715 … 0.046665900601739356 0.010500022518590856; 9.550968949453676 41.7945041477519 … 29.599186944629636 105.63062626379774])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function iterate(storage,pars)\n",
    "    se = zeros(7,8)\n",
    "    iter = 0\n",
    "    for x in 0:1\n",
    "        for y in 0:1\n",
    "            for z in 0:1\n",
    "                iter += 1\n",
    "                d = get_data(data,x,y,z) ### get_data(data,C,F,R)\n",
    "                res = optimize(x -> -log_likelihood_obj(d,pars,x),x0,BFGS(),Optim.Options(show_trace=false)) # find the MLE\n",
    "                pars = update(pars,res.minimizer) # update the parameters\n",
    "                storage[:,iter] = final_ests(res.minimizer,pars) # store the results\n",
    "                dF = ForwardDiff.jacobian(x -> final_ests(x,pars),res.minimizer) # compute the jacobian\n",
    "                var_est = dF * avar * dF' / N # compute the variance of the estimates using the Delta method\n",
    "                se[:,iter] = sqrt.(diag(var_est)) # store the standard errors\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return storage,se\n",
    "end\n",
    "                \n",
    "params, errors = iterate(zeros(7,8),pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, store the estimates and the standard errors in a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>7×17 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">param_names</th><th style = \"text-align: left;\">p_W_M_NC</th><th style = \"text-align: left;\">se_W_M_NC</th><th style = \"text-align: left;\">p_NW_M_NC</th><th style = \"text-align: left;\">se_NW_M_NC</th><th style = \"text-align: left;\">p_W_W_NC</th><th style = \"text-align: left;\">se_W_W_NC</th><th style = \"text-align: left;\">p_NW_W_NC</th><th style = \"text-align: left;\">se_NW_W_NC</th><th style = \"text-align: left;\">p_W_M_C</th><th style = \"text-align: left;\">se_W_M_C</th><th style = \"text-align: left;\">p_NW_M_C</th><th style = \"text-align: left;\">se_NW_M_C</th><th style = \"text-align: left;\">p_W_W_C</th><th style = \"text-align: left;\">se_W_W_C</th><th style = \"text-align: left;\">p_NW_W_C</th><th style = \"text-align: left;\">se_NW_W_C</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">h</td><td style = \"text-align: right;\">0.211386</td><td style = \"text-align: right;\">0.0131208</td><td style = \"text-align: right;\">0.15634</td><td style = \"text-align: right;\">0.0103814</td><td style = \"text-align: right;\">0.202006</td><td style = \"text-align: right;\">0.0126877</td><td style = \"text-align: right;\">0.149288</td><td style = \"text-align: right;\">0.00999598</td><td style = \"text-align: right;\">0.176418</td><td style = \"text-align: right;\">0.0114358</td><td style = \"text-align: right;\">0.164141</td><td style = \"text-align: right;\">0.0107986</td><td style = \"text-align: right;\">0.169579</td><td style = \"text-align: right;\">0.0110838</td><td style = \"text-align: right;\">0.158442</td><td style = \"text-align: right;\">0.0104947</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">δ</td><td style = \"text-align: right;\">0.0109654</td><td style = \"text-align: right;\">0.00105635</td><td style = \"text-align: right;\">0.0129843</td><td style = \"text-align: right;\">0.00124828</td><td style = \"text-align: right;\">0.00845501</td><td style = \"text-align: right;\">0.000816574</td><td style = \"text-align: right;\">0.0112654</td><td style = \"text-align: right;\">0.00108491</td><td style = \"text-align: right;\">0.00382837</td><td style = \"text-align: right;\">0.000371465</td><td style = \"text-align: right;\">0.0057423</td><td style = \"text-align: right;\">0.000556101</td><td style = \"text-align: right;\">0.00301676</td><td style = \"text-align: right;\">0.000292953</td><td style = \"text-align: right;\">0.00460016</td><td style = \"text-align: right;\">0.000446005</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">μ</td><td style = \"text-align: right;\">1.73575</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.97264</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.59919</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.60459</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.20915</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.65632</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">2.30222</td><td style = \"text-align: right;\">0.172534</td><td style = \"text-align: right;\">3.48832</td><td style = \"text-align: right;\">0.172534</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">σ</td><td style = \"text-align: right;\">1.03869</td><td style = \"text-align: right;\">0.0437437</td><td style = \"text-align: right;\">1.09247</td><td style = \"text-align: right;\">0.0460086</td><td style = \"text-align: right;\">1.09193</td><td style = \"text-align: right;\">0.0459856</td><td style = \"text-align: right;\">1.09158</td><td style = \"text-align: right;\">0.0459708</td><td style = \"text-align: right;\">1.10351</td><td style = \"text-align: right;\">0.0464734</td><td style = \"text-align: right;\">0.874308</td><td style = \"text-align: right;\">0.0368207</td><td style = \"text-align: right;\">1.15834</td><td style = \"text-align: right;\">0.0487827</td><td style = \"text-align: right;\">0.615712</td><td style = \"text-align: right;\">0.0259302</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">wres</td><td style = \"text-align: right;\">6.78993</td><td style = \"text-align: right;\">0.0572903</td><td style = \"text-align: right;\">3.80433e-10</td><td style = \"text-align: right;\">3.20992e-12</td><td style = \"text-align: right;\">4.53163e-11</td><td style = \"text-align: right;\">3.82358e-13</td><td style = \"text-align: right;\">6.75238e-11</td><td style = \"text-align: right;\">5.69735e-13</td><td style = \"text-align: right;\">21.8049</td><td style = \"text-align: right;\">0.18398</td><td style = \"text-align: right;\">15.3961</td><td style = \"text-align: right;\">0.129905</td><td style = \"text-align: right;\">10.1272</td><td style = \"text-align: right;\">0.0854488</td><td style = \"text-align: right;\">4.23881</td><td style = \"text-align: right;\">0.0357652</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">λ</td><td style = \"text-align: right;\">0.490083</td><td style = \"text-align: right;\">0.0772679</td><td style = \"text-align: right;\">0.15634</td><td style = \"text-align: right;\">0.0103814</td><td style = \"text-align: right;\">0.202006</td><td style = \"text-align: right;\">0.0126877</td><td style = \"text-align: right;\">0.149288</td><td style = \"text-align: right;\">0.00999598</td><td style = \"text-align: right;\">0.822674</td><td style = \"text-align: right;\">0.148083</td><td style = \"text-align: right;\">0.353336</td><td style = \"text-align: right;\">0.0630307</td><td style = \"text-align: right;\">0.342223</td><td style = \"text-align: right;\">0.0466659</td><td style = \"text-align: right;\">0.158513</td><td style = \"text-align: right;\">0.0105</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">b</td><td style = \"text-align: right;\">-142.312</td><td style = \"text-align: right;\">9.55097</td><td style = \"text-align: right;\">-308.191</td><td style = \"text-align: right;\">41.7945</td><td style = \"text-align: right;\">-365.998</td><td style = \"text-align: right;\">48.9209</td><td style = \"text-align: right;\">-224.947</td><td style = \"text-align: right;\">30.4314</td><td style = \"text-align: right;\">-501.512</td><td style = \"text-align: right;\">25.4956</td><td style = \"text-align: right;\">-284.677</td><td style = \"text-align: right;\">23.6099</td><td style = \"text-align: right;\">-496.743</td><td style = \"text-align: right;\">29.5992</td><td style = \"text-align: right;\">-576.753</td><td style = \"text-align: right;\">105.631</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& param\\_names & p\\_W\\_M\\_NC & se\\_W\\_M\\_NC & p\\_NW\\_M\\_NC & se\\_NW\\_M\\_NC & p\\_W\\_W\\_NC & se\\_W\\_W\\_NC & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & h & 0.211386 & 0.0131208 & 0.15634 & 0.0103814 & 0.202006 & 0.0126877 & $\\dots$ \\\\\n",
       "\t2 & δ & 0.0109654 & 0.00105635 & 0.0129843 & 0.00124828 & 0.00845501 & 0.000816574 & $\\dots$ \\\\\n",
       "\t3 & μ & 1.73575 & 0.172534 & 2.97264 & 0.172534 & 2.59919 & 0.172534 & $\\dots$ \\\\\n",
       "\t4 & σ & 1.03869 & 0.0437437 & 1.09247 & 0.0460086 & 1.09193 & 0.0459856 & $\\dots$ \\\\\n",
       "\t5 & wres & 6.78993 & 0.0572903 & 3.80433e-10 & 3.20992e-12 & 4.53163e-11 & 3.82358e-13 & $\\dots$ \\\\\n",
       "\t6 & λ & 0.490083 & 0.0772679 & 0.15634 & 0.0103814 & 0.202006 & 0.0126877 & $\\dots$ \\\\\n",
       "\t7 & b & -142.312 & 9.55097 & -308.191 & 41.7945 & -365.998 & 48.9209 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m7×17 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m param_names \u001b[0m\u001b[1m p_W_M_NC     \u001b[0m\u001b[1m se_W_M_NC  \u001b[0m\u001b[1m p_NW_M_NC      \u001b[0m\u001b[1m se_NW_M_NC   \u001b[0m\u001b[1m p_\u001b[0m ⋯\n",
       "     │\u001b[90m String      \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64        \u001b[0m\u001b[90m Float64      \u001b[0m\u001b[90m Fl\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ h               0.211386   0.0131208      0.15634       0.0103814       ⋯\n",
       "   2 │ δ               0.0109654  0.00105635     0.0129843     0.00124828\n",
       "   3 │ μ               1.73575    0.172534       2.97264       0.172534\n",
       "   4 │ σ               1.03869    0.0437437      1.09247       0.0460086\n",
       "   5 │ wres            6.78993    0.0572903      3.80433e-10   3.20992e-12     ⋯\n",
       "   6 │ λ               0.490083   0.0772679      0.15634       0.0103814\n",
       "   7 │ b            -142.312      9.55097     -308.191        41.7945       -3\n",
       "\u001b[36m                                                              12 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_out = DataFrame()\n",
    "\n",
    "param_names = [\"h\",\"δ\",\"μ\",\"σ\",\"wres\",\"λ\",\"b\"]\n",
    "\n",
    "df_out[!, :param_names] = param_names\n",
    "\n",
    "for i in 1:8\n",
    "    df_out[!, \"params$i\"] = params[:,i]\n",
    "    df_out[!, \"se$i\"] = errors[:,i]\n",
    "end\n",
    "C,F,R = [0,0,0] \n",
    "demographic_names = [\"W_M_NC\", \"NW_M_NC\", \"W_W_NC\", \"NW_W_NC\", \"W_M_C\", \"NW_M_C\", \"W_W_C\", \"NW_W_C\"]\n",
    "\n",
    "for (i, name) in enumerate(demographic_names)\n",
    "    rename!(df_out, \"params$i\" => \"p_$name\", \"se$i\" => \"se_$name\")\n",
    "end\n",
    "\n",
    "\n",
    "df_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
